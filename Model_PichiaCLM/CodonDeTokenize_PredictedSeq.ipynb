{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b19b1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "#from DataPrep import data_prep\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import tf_text to load the ops used by the tokenizer saved model\n",
    "#import tensorflow_text  # pylint: disable=unused-import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Model,  Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Bidirectional, Dropout, Input, TimeDistributed, Dense, Activation, RepeatVector, Embedding, Concatenate\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.layers import Attention\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)  # suppress warnings\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55d539d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cds_Pred = {}\n",
    "fname_list = [\"PichiaArch1\",\"PichiaArch1_HumanContext\",\"PichiaArch2\",\"PichiaArch2_HumanContext\"]\n",
    "N_seq = len(fname_list)\n",
    "for i in range(N_seq ):\n",
    "    Cds_Pred[i] = pd.read_excel('./PredictedSequences/' + fname_list[i]+'_Lactoferrin.xlsx').iloc[:,2:-1].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94514c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AA_Codon_list():\n",
    "    dic_AA_codon = {'A': ['GCT', 'GCC', 'GCA', 'GCG'],\n",
    "                    'C': ['TGT', 'TGC'],\n",
    "                    'D': ['GAT', 'GAC'],\n",
    "                    'E': ['GAA', 'GAG'],\n",
    "                    'F': ['TTT', 'TTC'],\n",
    "                    'G': ['GGT', 'GGA', 'GGC', 'GGG'],\n",
    "                    'H': ['CAT', 'CAC'],\n",
    "                    'I': ['ATT', 'ATC', 'ATA'],\n",
    "                    'K': ['AAA', 'AAG'],\n",
    "                    'L': ['TTA', 'TTG', 'CTT', 'CTC', 'CTA', 'CTG'],\n",
    "                    'M': ['ATG'],\n",
    "                    'N': ['AAT', 'AAC'],\n",
    "                    'P': ['CCT', 'CCC', 'CCA', 'CCG'],\n",
    "                    'Q': ['CAA', 'CAG'],\n",
    "                    'R': ['CGT', 'CGC', 'CGA', 'CGG', 'AGA', 'AGG'],\n",
    "                    'S': ['TCT', 'TCC', 'TCA', 'TCG', 'AGT', 'AGC'],\n",
    "                    'T': ['ACT', 'ACC', 'ACA', 'ACG'],\n",
    "                    'V': ['GTT', 'GTC', 'GTA', 'GTG'],\n",
    "                    'W': ['TGG'],\n",
    "                    'Y': ['TAT', 'TAC'],\n",
    "                    '*': ['TAA', 'TAG', 'TGA']}\n",
    "\n",
    "    codon_list = []\n",
    "    AA_list = []\n",
    "    for key in dic_AA_codon:\n",
    "        for i in range(len(dic_AA_codon[key])):\n",
    "            AA_list.append(key)\n",
    "        for i in dic_AA_codon[key]:\n",
    "            codon_list.append(i)\n",
    "    return AA_list, codon_list\n",
    "\n",
    "def de_tokenize_Codon(sequences):\n",
    "    AA_list, codon_list = AA_Codon_list()\n",
    "    keys = codon_list\n",
    "    values = range(1, len(codon_list) + 1)\n",
    "    Codon_dict = dict(zip(keys, values))\n",
    "    seq_de_tokenized = []\n",
    "    for s in range(len(sequences)):\n",
    "        seq = sequences[s]\n",
    "        temp = ''\n",
    "        for i in range(len(seq)):\n",
    "             temp = temp+keys[values.index(seq[i])]\n",
    "        \n",
    "\n",
    "        seq_de_tokenized.append(temp)\n",
    "\n",
    "    return seq_de_tokenized, Codon_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a0eb4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cds_seq_de_tokenized = {}\n",
    "for i in range(N_seq):\n",
    "    Cds_seq_de_tokenized[i], Cds_seq_tokenizer = de_tokenize_Codon(Cds_Pred[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ede010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Cds_seq_de_tokenized).to_csv('DeTokenized_Lactoferrin_PredictedSeq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a750cf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_format = {}\n",
    "for i in range(len(fname_list)):\n",
    "    fasta_format[i] = \">Lactoferrin Homo Sapiens \" + fname_list[i]+\"\\n\"\n",
    "    n_frags_60 = int(np.ceil(len(Cds_seq_de_tokenized[i][0])/60))\n",
    "    for j in range (n_frags_60):\n",
    "        fasta_format[i] = fasta_format[i] + Cds_seq_de_tokenized[i][0][j*60: (j+1)*60] + \"\\n\"\n",
    "    \n",
    "    saveFasta = open(r'PredSeq_'+ fname_list[i]+'_DNA.fasta', 'w+')\n",
    "    saveFasta.write(fasta_format[i])\n",
    "    saveFasta.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ac727b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fname_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe470fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
