{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b19b1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "#from DataPrep import data_prep\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import tf_text to load the ops used by the tokenizer saved model\n",
    "#import tensorflow_text  # pylint: disable=unused-import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Model,  Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Bidirectional, Dropout, Input, TimeDistributed, Dense, Activation, RepeatVector, Embedding, Concatenate\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.layers import Attention\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)  # suppress warnings\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1b4cbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/harini/MIT Dropbox/Harini Narayanan/Harini Narayananâ€™s files/1_Home/CodonOptimization/CO_Application/h_tPA_IL12\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18e99869",
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq_Data = pd.read_excel('h_tPA_IL12.xlsx')\n",
    "AA_ts_seq_v0 = (Seq_Data.iloc[:,2] + Seq_Data.iloc[:,1]).tolist()\n",
    "AA_ts_seq = []\n",
    "for seq in AA_ts_seq_v0:\n",
    "    AA_ts_seq.append(seq+'*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bf7c10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>CDS</th>\n",
       "      <th>SP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tPA</td>\n",
       "      <td>SQEIHARFRRGARSYQVICRDEKTQMIYQQHQSWLRPVLRSNRVEY...</td>\n",
       "      <td>MDAMKRGLCCVLLLCGAVFVSP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IL12p40 (IL12B)</td>\n",
       "      <td>IWELKKDVYVVELDWYPDAPGEMVVLTCDTPEEDGITWTLDQSSEV...</td>\n",
       "      <td>MCHQQLVISWFSLVFLASPLVA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IL12p35 (IL12A)</td>\n",
       "      <td>RNLPVATPDPGMFPCLHHSQNLLRAVSNMLQKARQTLEFYPCTSEE...</td>\n",
       "      <td>MCPARSLLLVATLVLLDHLSLA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name                                                CDS  \\\n",
       "0              tPA  SQEIHARFRRGARSYQVICRDEKTQMIYQQHQSWLRPVLRSNRVEY...   \n",
       "1  IL12p40 (IL12B)  IWELKKDVYVVELDWYPDAPGEMVVLTCDTPEEDGITWTLDQSSEV...   \n",
       "2  IL12p35 (IL12A)  RNLPVATPDPGMFPCLHHSQNLLRAVSNMLQKARQTLEFYPCTSEE...   \n",
       "\n",
       "                       SP  \n",
       "0  MDAMKRGLCCVLLLCGAVFVSP  \n",
       "1  MCHQQLVISWFSLVFLASPLVA  \n",
       "2  MCPARSLLLVATLVLLDHLSLA  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Seq_Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "437c2789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_AA(sequences):\n",
    "    AA_dict = {'A': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'K': 9, 'L': 10,\n",
    "               'M': 11, 'N': 12, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'V': 18, 'W': 19,\n",
    "               'Y': 20, 'X': 21, 'Z': 21, 'B': 21, 'U': 21, 'O': 21, '*': 22}\n",
    "\n",
    "    seq_tokenized = []\n",
    "    for s in range(len(sequences)):\n",
    "        seq = sequences[s]\n",
    "        temp = [24]  # Start token\n",
    "        for i in range(len(seq)):\n",
    "            temp.append(AA_dict[seq[i]])\n",
    "        temp.append(23)\n",
    "\n",
    "        seq_tokenized.append(temp)\n",
    "\n",
    "    return seq_tokenized, AA_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b129485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AA_Codon_list():\n",
    "    dic_AA_codon = {'A': ['GCT', 'GCC', 'GCA', 'GCG'],\n",
    "                    'C': ['TGT', 'TGC'],\n",
    "                    'D': ['GAT', 'GAC'],\n",
    "                    'E': ['GAA', 'GAG'],\n",
    "                    'F': ['TTT', 'TTC'],\n",
    "                    'G': ['GGT', 'GGA', 'GGC', 'GGG'],\n",
    "                    'H': ['CAT', 'CAC'],\n",
    "                    'I': ['ATT', 'ATC', 'ATA'],\n",
    "                    'K': ['AAA', 'AAG'],\n",
    "                    'L': ['TTA', 'TTG', 'CTT', 'CTC', 'CTA', 'CTG'],\n",
    "                    'M': ['ATG'],\n",
    "                    'N': ['AAT', 'AAC'],\n",
    "                    'P': ['CCT', 'CCC', 'CCA', 'CCG'],\n",
    "                    'Q': ['CAA', 'CAG'],\n",
    "                    'R': ['CGT', 'CGC', 'CGA', 'CGG', 'AGA', 'AGG'],\n",
    "                    'S': ['TCT', 'TCC', 'TCA', 'TCG', 'AGT', 'AGC'],\n",
    "                    'T': ['ACT', 'ACC', 'ACA', 'ACG'],\n",
    "                    'V': ['GTT', 'GTC', 'GTA', 'GTG'],\n",
    "                    'W': ['TGG'],\n",
    "                    'Y': ['TAT', 'TAC'],\n",
    "                    '*': ['TAA', 'TAG', 'TGA']}\n",
    "\n",
    "    codon_list = []\n",
    "    AA_list = []\n",
    "    for key in dic_AA_codon:\n",
    "        for i in range(len(dic_AA_codon[key])):\n",
    "            AA_list.append(key)\n",
    "        for i in dic_AA_codon[key]:\n",
    "            codon_list.append(i)\n",
    "    return AA_list, codon_list\n",
    "\n",
    "def tokenize_Codon(sequences):\n",
    "    AA_list, codon_list = AA_Codon_list()\n",
    "    keys = codon_list\n",
    "    values = range(1, len(codon_list) + 1)\n",
    "    Codon_dict = dict(zip(keys, values))\n",
    "    seq_tokenized = []\n",
    "    for s in range(len(sequences)):\n",
    "        seq = sequences[s]\n",
    "        temp = [65]  # Start token\n",
    "        for i in range(int(len(seq) / 3)):\n",
    "            temp.append(Codon_dict[seq[3 * i: 3 * (i + 1)]])\n",
    "        \n",
    "        temp.append(66)\n",
    "        seq_tokenized.append(temp)\n",
    "\n",
    "    return seq_tokenized, Codon_dict\n",
    "\n",
    "def tokenize_AA(sequences):\n",
    "    AA_dict = {'A': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'K': 9, 'L': 10,\n",
    "               'M': 11, 'N': 12, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'V': 18, 'W': 19,\n",
    "               'Y': 20, 'X': 21, 'Z': 21, 'B': 21, 'U': 21, 'O': 21, '*': 22}\n",
    "\n",
    "    seq_tokenized = []\n",
    "    for s in range(len(sequences)):\n",
    "        seq = sequences[s]\n",
    "        temp = [24]  # Start token\n",
    "        for i in range(len(seq)):\n",
    "            temp.append(AA_dict[seq[i]])\n",
    "        temp.append(23)\n",
    "\n",
    "        seq_tokenized.append(temp)\n",
    "\n",
    "    return seq_tokenized, AA_dict\n",
    "\n",
    "\n",
    "def tokenized_AACodonList():\n",
    "    AA_dict = {'A': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'K': 9, 'L': 10,\n",
    "               'M': 11, 'N': 12, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'V': 18, 'W': 19,\n",
    "               'Y': 20, 'X': 21, 'Z': 21, 'B': 21, 'U': 21, 'O': 21, '*': 22}\n",
    "    dic_AA_codon = {'A': ['GCT', 'GCC', 'GCA', 'GCG'],\n",
    "                    'C': ['TGT', 'TGC'],\n",
    "                    'D': ['GAT', 'GAC'],\n",
    "                    'E': ['GAA', 'GAG'],\n",
    "                    'F': ['TTT', 'TTC'],\n",
    "                    'G': ['GGT', 'GGA', 'GGC', 'GGG'],\n",
    "                    'H': ['CAT', 'CAC'],\n",
    "                    'I': ['ATT', 'ATC', 'ATA'],\n",
    "                    'K': ['AAA', 'AAG'],\n",
    "                    'L': ['TTA', 'TTG', 'CTT', 'CTC', 'CTA', 'CTG'],\n",
    "                    'M': ['ATG'],\n",
    "                    'N': ['AAT', 'AAC'],\n",
    "                    'P': ['CCT', 'CCC', 'CCA', 'CCG'],\n",
    "                    'Q': ['CAA', 'CAG'],\n",
    "                    'R': ['CGT', 'CGC', 'CGA', 'CGG', 'AGA', 'AGG'],\n",
    "                    'S': ['TCT', 'TCC', 'TCA', 'TCG', 'AGT', 'AGC'],\n",
    "                    'T': ['ACT', 'ACC', 'ACA', 'ACG'],\n",
    "                    'V': ['GTT', 'GTC', 'GTA', 'GTG'],\n",
    "                    'W': ['TGG'],\n",
    "                    'Y': ['TAT', 'TAC'],\n",
    "                    '*': ['TAA', 'TAG', 'TGA']}\n",
    "    AA_list, codon_list = AA_Codon_list()\n",
    "    token_AA_codon = {}\n",
    "    for i in dic_AA_codon.keys():\n",
    "        token_AA_codon[AA_dict[i]] = []\n",
    "        for j in dic_AA_codon[i]:\n",
    "            token_AA_codon[AA_dict[i]].append(codon_list.index(j)+1)\n",
    "            \n",
    "    return token_AA_codon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dae347dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "AA_list, codon_list = AA_Codon_list()\n",
    "token_AA_codon = tokenized_AACodonList()\n",
    "token_AA_codon[23] = [66]\n",
    "AA_seq_tokenized, AA_seq_tokenizer = tokenize_AA(AA_ts_seq)\n",
    "AA_pad_seq = pad_sequences(AA_seq_tokenized, maxlen=1001, dtype='int32', padding=\"post\", truncating=\"post\")\n",
    "\n",
    "AA_ts = AA_pad_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a998396",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d1cc477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/harini/MIT Dropbox/Harini Narayanan/Harini Narayananâ€™s files/1_Home/CodonOptimization/CO_Application\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a01d01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6719b164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 19:55:05.746547: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 1000)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 1000, 42)     1050        ['input_1[0][0]',                \n",
      "                                                                  'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 999)]        0           []                               \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  [(None, 1000, 1020)  1695240     ['embedding[0][0]']              \n",
      "                                , (None, 510),                                                    \n",
      "                                 (None, 510)]                                                     \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 1000)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 999, 224)     15008       ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 1020)         0           ['bidirectional[0][1]',          \n",
      "                                                                  'bidirectional[0][2]']          \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                    [(None, 999, 1020),  3812760     ['embedding_1[0][0]',            \n",
      "                                 (None, 1020)]                    'concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " gru_2 (GRU)                    [(None, 1000, 1020)  3255840     ['embedding[1][0]',              \n",
      "                                , (None, 1020)]                   'concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " attention (Attention)          (None, 999, 1020)    0           ['gru_1[0][0]',                  \n",
      "                                                                  'bidirectional[0][0]']          \n",
      "                                                                                                  \n",
      " attention_1 (Attention)        (None, 1000, 1020)   0           ['gru_2[0][0]',                  \n",
      "                                                                  'bidirectional[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 999, 2040)    0           ['gru_1[0][0]',                  \n",
      "                                                                  'attention[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 1000, 2040)   0           ['gru_2[0][0]',                  \n",
      "                                                                  'attention_1[0][0]']            \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, 999, 125)    255125      ['concatenate_1[0][0]']          \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDistri  (None, 1000, 139)   283699      ['concatenate_2[0][0]']          \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " dropout (Dropout)              multiple             0           ['time_distributed[0][0]',       \n",
      "                                                                  'time_distributed_1[0][0]']     \n",
      "                                                                                                  \n",
      " time_distributed_2 (TimeDistri  (None, 999, 67)     8442        ['dropout[0][0]']                \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " time_distributed_3 (TimeDistri  (None, 1000, 25)    3500        ['dropout[1][0]']                \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,330,664\n",
      "Trainable params: 9,330,664\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 1000)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 1000, 42)     1050        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  [(None, 1000, 1020)  1695240     ['embedding[0][0]']              \n",
      "                                , (None, 510),                                                    \n",
      "                                 (None, 510)]                                                     \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 1020)         0           ['bidirectional[0][1]',          \n",
      "                                                                  'bidirectional[0][2]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,696,290\n",
      "Trainable params: 1,696,290\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 1000)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 1000, 42)     1050        ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 1020)]       0           []                               \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " gru_2 (GRU)                    [(None, 1000, 1020)  3255840     ['embedding[2][0]',              \n",
      "                                , (None, 1020)]                   'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, 1000, 1020)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " attention_1 (Attention)        (None, 1000, 1020)   0           ['gru_2[1][0]',                  \n",
      "                                                                  'input_5[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 1000, 2040)   0           ['gru_2[1][0]',                  \n",
      "                                                                  'attention_1[1][0]']            \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDistri  (None, 1000, 139)   283699      ['concatenate_2[1][0]']          \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " dropout (Dropout)              multiple             0           ['time_distributed_1[1][0]']     \n",
      "                                                                                                  \n",
      " time_distributed_3 (TimeDistri  (None, 1000, 25)    3500        ['dropout[2][0]']                \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,544,089\n",
      "Trainable params: 3,544,089\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        multiple             15008       ['input_8[0][0]']                \n",
      "                                                                                                  \n",
      " input_7 (InputLayer)           [(None, 1020)]       0           []                               \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                    multiple             3812760     ['embedding_1[1][0]',            \n",
      "                                                                  'input_7[0][0]']                \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 1000, 1020)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " attention (Attention)          multiple             0           ['gru_1[1][0]',                  \n",
      "                                                                  'input_6[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 1, 2040)      0           ['gru_1[1][0]',                  \n",
      "                                                                  'attention[1][0]']              \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  multiple            255125      ['concatenate_3[0][0]']          \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      " dropout (Dropout)              multiple             0           ['time_distributed[1][0]']       \n",
      "                                                                                                  \n",
      " time_distributed_2 (TimeDistri  multiple            8442        ['dropout[3][0]']                \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,091,335\n",
      "Trainable params: 4,091,335\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Settings = pd.read_csv('../Models/2Target_Pichia/BO_forHyperParameter/Arch1/Round3.csv').iloc[:, 1:]\n",
    "Setting_no =1\n",
    "\n",
    "Max_length = 1000\n",
    "learning_rate = 0.001\n",
    "batch_size = 150\n",
    "epochs = 100\n",
    "aa_vocab_size = 25\n",
    "dna_vocab_size = 67\n",
    "\n",
    "\n",
    "hidden_size_enc = int(Settings['Enc hidden size'][Setting_no])\n",
    "hidden_size_enc_aa = int(Settings['Enc hidden size'][Setting_no])\n",
    "embedding_size_enc = int(Settings['Enc Embedding size'][Setting_no])\n",
    "embedding_size_dec = int(Settings['Dec Embedding size'][Setting_no])\n",
    "Dense_layer_size = int(Settings['Dense Layer size'][Setting_no])\n",
    "Dense_layer_size_aa = int(Settings['Dense Layer size aa'][Setting_no])\n",
    "\n",
    "drop_rate = Settings['Drop rate'][Setting_no]\n",
    "drop_rate_aa = Settings['Drop rate aa'][Setting_no]\n",
    "\n",
    "    \n",
    "input_sequence = Input(shape=(Max_length,))\n",
    "encod_emb = Embedding(input_dim= aa_vocab_size, output_dim = embedding_size_enc,trainable=True, mask_zero = True)\n",
    "embedding = encod_emb(input_sequence)\n",
    "\n",
    "encoder = Bidirectional(GRU(hidden_size_enc, return_sequences=True, return_state = True),\n",
    "                        merge_mode=\"concat\", weights=None)\n",
    "\n",
    "encoder_sequence, encoder_final_f, encoder_final_b  = encoder(embedding)\n",
    "\n",
    "encoder_final = Concatenate(axis=-1)([encoder_final_f, encoder_final_b])\n",
    "\n",
    "\n",
    "\n",
    "decoder_inputs = Input(shape=(Max_length -1, ))\n",
    "decoder_inputs_aa = Input(shape=(Max_length, ))\n",
    "\n",
    "dex=  Embedding(input_dim = dna_vocab_size, output_dim = embedding_size_dec, trainable=True, mask_zero = True)\n",
    "\n",
    "\n",
    "final_dex= dex(decoder_inputs)\n",
    "final_dex_aa =  encod_emb(decoder_inputs_aa)\n",
    "\n",
    "\n",
    "decoder = GRU(2*hidden_size_enc, return_sequences = True, return_state = True)\n",
    "decoder_aa =  GRU(2*hidden_size_enc_aa, return_sequences = True, return_state = True)\n",
    "\n",
    "decoder_sequence, decoder_final = decoder(final_dex, initial_state=encoder_final)\n",
    "decoder_sequence_aa, decoder_final_aa = decoder_aa(final_dex_aa, initial_state=encoder_final)\n",
    "\n",
    "\n",
    "attn_layer = Attention()\n",
    "attn_out = attn_layer([decoder_sequence, encoder_sequence])\n",
    "attn_layer_aa = Attention()\n",
    "attn_out_aa = attn_layer_aa([decoder_sequence_aa, encoder_sequence])\n",
    "\n",
    "decoder_concat_input = Concatenate(axis=-1)([decoder_sequence, attn_out]) #decoder_sequence, \n",
    "decoder_concat_input_aa = Concatenate(axis=-1)([decoder_sequence_aa, attn_out_aa]) #decoder_sequence,\n",
    "\n",
    "\n",
    "Intermediate_layer = TimeDistributed(Dense(Dense_layer_size, activation='tanh'))\n",
    "Intermediate_layer_aa= TimeDistributed(Dense(Dense_layer_size_aa, activation='tanh'))\n",
    "\n",
    "Intemediate_output = Intermediate_layer(decoder_concat_input) #decoder_concat_input\n",
    "Intemediate_output_aa = Intermediate_layer_aa(decoder_concat_input_aa) #decoder_concat_input\n",
    "\n",
    "\n",
    "dropout_layer = Dropout(drop_rate)\n",
    "dropout_output = dropout_layer(Intemediate_output)\n",
    "\n",
    "dropout_layer_aa = Dropout(drop_rate_aa)\n",
    "dropout_output_aa = dropout_layer(Intemediate_output_aa)\n",
    "\n",
    "dense_layer = TimeDistributed(Dense(dna_vocab_size, activation='softmax'))\n",
    "logits = dense_layer(dropout_output)\n",
    "\n",
    "dense_layer_aa = TimeDistributed(Dense(aa_vocab_size, activation='softmax'))\n",
    "logits_aa = dense_layer_aa(dropout_output_aa)\n",
    "\n",
    "enc_dec_model = Model([input_sequence, decoder_inputs, decoder_inputs_aa], [logits, logits_aa])\n",
    "\n",
    "enc_dec_model.compile(loss=sparse_categorical_crossentropy,\n",
    "              optimizer=Adam(learning_rate = learning_rate),\n",
    "              metrics=['accuracy'])\n",
    "enc_dec_model.summary()\n",
    "\n",
    "enc_dec_model.load_weights(\"../Models/OptimizedModel_Checkpoints/PichiaData/2Target_AllData/FinArch1_AttnCorr_cp.ckpt\")\n",
    "\n",
    "after = encod_emb.get_weights()[0]\n",
    "\n",
    "encoder_model = Model(input_sequence, [encoder_final, encoder_sequence])\n",
    "encoder_model.summary()\n",
    "\n",
    "\n",
    "decoder_model_aa = Model([decoder_inputs_aa, encoder_final, encoder_sequence], [logits_aa])\n",
    "\n",
    "decoder_model_aa.summary()\n",
    "\n",
    "encoder_sequence = Input(shape = (Max_length, 2*hidden_size_enc))\n",
    "\n",
    "\n",
    "decoder_states_inputs = Input(shape = (2*hidden_size_enc,))\n",
    "decoder_inputs_inf = Input(shape = (1,)) \n",
    "final_dex2= dex(decoder_inputs_inf)\n",
    "\n",
    "decoder_outputs2, decoder_states2 = decoder(final_dex2, initial_state=decoder_states_inputs)\n",
    "\n",
    "attn_out_inf = attn_layer([decoder_outputs2, encoder_sequence])\n",
    "\n",
    "decoder_concat_input_inf = Concatenate(axis=-1)([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "decoder_out_fin = dense_layer(dropout_layer(Intermediate_layer(decoder_concat_input_inf)))\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs_inf, decoder_states_inputs, encoder_sequence],\n",
    "    [decoder_out_fin, decoder_states2])\n",
    "\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a91b9f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_cds_sequence(input_seq):\n",
    "    #encode the input as state vectors\n",
    "    batch_size = 1\n",
    "    dna_vocab = 67\n",
    "    \n",
    "    final_value, sequence_enc = encoder_model.predict(input_seq)\n",
    "    \n",
    "    target_seq = np.zeros((1,1))\n",
    "    #populate the first character of target sequence with the start character\n",
    "    target_seq[0, 0] = 65\n",
    "\n",
    "    states_value = final_value\n",
    "    stop_condition = False\n",
    "    decoded_sentence = [65] #''\n",
    "    counter = 0\n",
    "    loop_check = 0\n",
    "    \n",
    "    while not stop_condition:\n",
    "        output_tokens, h = decoder_model.predict([target_seq, states_value, sequence_enc])\n",
    "        Mask = np.zeros((1, dna_vocab))\n",
    "        Mask[0, token_AA_codon[input_seq.tolist()[0][counter]]] = 1\n",
    "        output_tokens_modified = np.multiply(output_tokens, Mask)\n",
    "        \n",
    "        sampled_token_index = np.argmax(output_tokens_modified )\n",
    "        decoded_sentence.append(sampled_token_index)\n",
    "        counter = counter + 1\n",
    "        appended_index = sampled_token_index\n",
    "  \n",
    "        if (len(decoded_sentence) >= Max_length or appended_index == 66):\n",
    "            stop_condition = True\n",
    "        \n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        #update states\n",
    "        states_value = h\n",
    "        \n",
    "    \n",
    "        \n",
    "    print(counter)\n",
    "    return decoded_sentence\n",
    "\n",
    "def decode_aa_sequence(input_seq):\n",
    "    \n",
    "    final_value, sequence_enc = encoder_model.predict(input_seq)\n",
    "    output_tokens = decoder_model_aa.predict([input_seq ,final_value, sequence_enc])\n",
    "    \n",
    "    decoded_sentence = []\n",
    "    for i in range(len(input_seq[0])):\n",
    "        sampled_token_index = np.argmax(output_tokens[0,i,:])\n",
    "        decoded_sentence.append(sampled_token_index)\n",
    "        if sampled_token_index ==23:\n",
    "            break\n",
    "    \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9920414c",
   "metadata": {},
   "source": [
    "#### CDS prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "622d587d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/harini/MIT Dropbox/Harini Narayanan/Harini Narayananâ€™s files/1_Home/CodonOptimization/CO_Application/h_tPA_IL12\n"
     ]
    }
   ],
   "source": [
    "cd h_tPA_IL12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1af44a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 19:55:13.904401: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"110\" frequency: 2400 num_cores: 16 environment { key: \"cpu_instruction_set\" value: \"SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 16777216 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "564\n",
      "330\n",
      "221\n"
     ]
    }
   ],
   "source": [
    "encoder_input_data = AA_ts\n",
    "cds_ts_predicted = []\n",
    "offset = 0\n",
    "\n",
    "for seq_index in np.arange(offset+0,len(AA_ts)):#len(AA_ts)  2500,len(AA_ts), offset+500\n",
    "    #take one sequence (part of the training set) for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1, 1:Max_length+1]\n",
    "    decoded_sentence = decode_cds_sequence(input_seq)\n",
    "    cds_ts_predicted.append(decoded_sentence)\n",
    "    \n",
    "\n",
    "\n",
    "pd.DataFrame(cds_ts_predicted).to_excel(\"./PredictedSequences/PichiaArch1_h_tPA_IL12.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5867aecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "565"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cds_ts_predicted[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0252b61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 19:55:56.449706: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"110\" frequency: 2400 num_cores: 16 environment { key: \"cpu_instruction_set\" value: \"SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 16777216 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    }
   ],
   "source": [
    "encoder_input_data = AA_ts\n",
    "aa_ts_predicted = []\n",
    "\n",
    "for seq_index in np.arange(0, len(AA_ts)):#len(AA_ts)  2500,len(AA_ts)\n",
    "    decoded_sentence_aa = decode_aa_sequence(encoder_input_data[seq_index: seq_index + 1,0:Max_length])\n",
    "    aa_ts_predicted.append(decoded_sentence_aa)\n",
    "    \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = []\n",
    "acc_aa = []\n",
    "for i in np.arange(0, len(AA_ts)): #len(AA_ts) - 2500\n",
    "   \n",
    "    obs_aa = [x for x in AA_ts[i, 1:Max_length+1] if x]\n",
    "    \n",
    "    if len(obs_aa) == len(aa_ts_predicted[i]):\n",
    "        acc_aa.append(accuracy_score(obs_aa, aa_ts_predicted[i]))\n",
    "        #print(accuracy_score(obs, cds_ts_predicted[i]))\n",
    "    else:\n",
    "        acc_aa.append(np.nan)\n",
    "        #print(NaN)\n",
    "\n",
    "#pd.DataFrame(acc_aa).to_excel('AA_Check.xlsx','PichiaArch1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f85b1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(acc_aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a750cf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcca0758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
